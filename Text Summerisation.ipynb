{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df9ba7c9",
   "metadata": {},
   "source": [
    "# An Investigation Into Different Approaches For Summerisation Using NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edb62be",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "Text summarization in NLP is the process of summarising the large body of text into smaller chunk suitable which is more suitable for the readers comprehension. In this article, I will investigate different approaches to summerisation from traditional to advance methods such as generative AI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bab445c",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Summerisation of large body of text has been an essential need, during our daily lives when we are working or reading, we realise a lot of information we are reading can be summerised to a smaller more digestable format, we also have issue of needing to read a document but lacking the time as we are too busy with other matter. This could rnage from newspapers, articles, research papers, books etc. \n",
    "\n",
    "What I want to accomplish through this investigation is to discover the most cost-effective approach to summerisation and integrate this feature to our AI learning assistant we are developing. We will be researching and documenting various techniques used for summerisation and choose the most suitable approach after rigourously evaluating the method. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec31a1c",
   "metadata": {},
   "source": [
    "## Types of Text Summarization\n",
    "After an extensive investigation finding numerous summerisation techniques we have categorised the two different types of approaches to text summerisation, these are the approaches, Extractive and Abstractive\n",
    "\n",
    "### Extractive Text Summarization\n",
    "It is the traditional method developed first. The main objective is to identify the significant sentences of the text and add them to the summary. You need to note that the summary obtained contains exact sentences from the original text.\n",
    "\n",
    "### Abstractive Text Summarization\n",
    "It is a more advanced method, with many advancements coming out frequently. The approach is to identify the important sections, interpret the context, and reproduce in a new way. This ensures that the core information is conveyed through the shortest text possible. Here, the sentences in the summary are generated, not just extracted from the original text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5746738",
   "metadata": {},
   "source": [
    "## Text Summarization using Gensim with TextRank\n",
    "TextRank is a graph-based ranking algorithm specifically adapted for text processing. It is similar to LexRank in that it is based on the concept of ranking sentences for importance within the text. The core idea is that sentences \"recommend\" each other, much like web pages do in Google's PageRank algorithm. Each sentence is treated as a node, and the connections between them are based on their similarity. A voting or recommendation process occurs where the importance of a sentence is determined by the importance of the sentences recommending it.\n",
    "\n",
    "As a result of this iterative voting or recommendation process, TextRank identifies sentences that are central to the text and thus should be included in the summary. The sentences chosen for the summary are those that are most highly ranked according to this algorithm.\n",
    "\n",
    "TextRank excels when applied to texts that have a dense web of semantic similarities between sentences, such as scientific articles, technical papers, and legal documents. It is particularly useful for documents where the use of domain-specific vocabulary leads to clear patterns of word use within the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27b8b2d",
   "metadata": {},
   "source": [
    "### Step 1: Add all neccessary installation and choose a File\n",
    "Add all neccessary imports and create function to prompt the user to select a PDF file from their system using a file dialog.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cddb7f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pdfminer.six nltk networkx gensim tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffd431a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "\n",
    "def choose_file():\n",
    "    # Initialize the Tkinter GUI\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()  # Hide the root window\n",
    "    file_path = filedialog.askopenfilename(filetypes=[(\"PDF files\", \"*.pdf\")])  # Only allow PDFs\n",
    "    return file_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4b4f9c",
   "metadata": {},
   "source": [
    "### Step 2: Text Extraction and Preprocessing\n",
    "\n",
    "The following functions are used to extract text from a PDF and preprocess it for summarization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "460a0104",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    return extract_text(pdf_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76ae85e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    # Split text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # Tokenize, stem, and remove stop words\n",
    "    preprocessed_text = []\n",
    "    for sentence in sentences:\n",
    "        tokens = [stemmer.stem(word) for word in word_tokenize(sentence.lower())\n",
    "                  if word not in stop_words and word.isalnum()]\n",
    "        if tokens:  # Only add non-empty lists\n",
    "            preprocessed_text.append(tokens)\n",
    "    \n",
    "    # Filter out sentences that correspond to empty preprocessed lists\n",
    "    sentences = [sentences[i] for i, tokens in enumerate(preprocessed_text) if tokens]\n",
    "    \n",
    "    return sentences, preprocessed_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cba2b42",
   "metadata": {},
   "source": [
    "### Step 3: Implement TextRank\n",
    "\n",
    "We will now define functions to build a similarity matrix using Word2Vec and apply the TextRank algorithm to rank sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3fef8a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "def build_similarity_matrix(sentences, preprocessed_text):\n",
    "    # Train a Word2Vec model\n",
    "    if preprocessed_text:\n",
    "        model = Word2Vec(preprocessed_text, vector_size=100, window=2, min_count=1, workers=2)\n",
    "        model_wv = model.wv\n",
    "    else:\n",
    "        raise ValueError(\"The preprocessed text is empty. Please check your preprocessing steps.\")\n",
    "    \n",
    "    # Create an empty similarity matrix\n",
    "    sim_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "    \n",
    "    # Build the similarity matrix\n",
    "    for i in range(len(sentences)):\n",
    "        for j in range(len(sentences)):\n",
    "            if i != j:\n",
    "                # Ensure each sentence has at least one word after preprocessing\n",
    "                if preprocessed_text[i] and preprocessed_text[j]:\n",
    "                    vector_i = np.mean([model_wv[word] for word in preprocessed_text[i] if word in model_wv], axis=0)\n",
    "                    vector_j = np.mean([model_wv[word] for word in preprocessed_text[j] if word in model_wv], axis=0)\n",
    "                    \n",
    "                    # Check that vectors are valid (not NaN or infinite)\n",
    "                    if not np.all(np.isfinite(vector_i)) or not np.all(np.isfinite(vector_j)):\n",
    "                        continue\n",
    "                    \n",
    "                    sim_matrix[i][j] = cosine_similarity([vector_i], [vector_j])[0, 0]\n",
    "    \n",
    "    return sim_matrix\n",
    "\n",
    "\n",
    "def apply_text_rank(sim_matrix, sentences):\n",
    "    nx_graph = nx.from_numpy_array(sim_matrix)\n",
    "    scores = nx.pagerank(nx_graph, max_iter=10000, tol=1e-3)\n",
    "    ranked_sentences = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)\n",
    "    return ranked_sentences\n",
    "\n",
    "def summarize(pdf_path, num_sentences=5):\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    sentences, preprocessed_text = preprocess_text(text)\n",
    "    sim_matrix = build_similarity_matrix(sentences, preprocessed_text)\n",
    "    ranked_sentences = apply_text_rank(sim_matrix, sentences)\n",
    "    summary = ' '.join([s[1] for s in ranked_sentences[:num_sentences]])\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e81d1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. But\n",
      "along with honor comes, or must come, the charismatic leader’s recognition of his\n",
      "‘Eigenverantwortung’  (‘Self-responsibility’)  (1992b:  180). For  him,  both  Stefan  George  and  Tolstoy  were  charismatic  leaders  who  were\n",
      "‘irrational’. For  Weber,  bureaucratic  authority  has  many  positive  features:  it  is  based\n",
      "upon reason, it is impartially implemented by paid trained ofﬁcials, and its future\n",
      "\n",
      " \n",
      "\f",
      "is stable. One of the differences between bureaucratic and traditional Herrschaft, if not the key one is that\n",
      "the former is based upon the concept of ‘competence’, which is lacking in the latter (see Weber,\n",
      "1988: 478, 482).\n"
     ]
    }
   ],
   "source": [
    "pdf_path = choose_file()\n",
    "summary = summarize(pdf_path)\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44248b6b",
   "metadata": {},
   "source": [
    "## PDF Summarization Using LexRank\n",
    "\n",
    "LexRank is an unsupervised approach to text summarization based on graph theory. Sentences within a given text are represented as vertices in a graph. Edges between sentences are created based on the similarity between sentences, which can be computed using measures like cosine similarity with TF-IDF weighting. The LexRank algorithm then applies a method similar to Google's PageRank to this graph: a sentence is considered important if it is similar to many other sentences, and those sentences are themselves considered important.\n",
    "\n",
    "LexRank is an unsupervised approach to text summarization based on graph theory. Sentences within a given text are represented as vertices in a graph. Edges between sentences are created based on the similarity between sentences, which can be computed using measures like cosine similarity with TF-IDF weighting. The LexRank algorithm then applies a method similar to Google's PageRank to this graph: a sentence is considered important if it is similar to many other sentences, and those sentences are themselves considered important.\n",
    "\n",
    " LexRank is particularly effective on structured and well-written texts where the salient information is distributed throughout the document. It works well with news articles, research papers, and technical documents, where the recurrence of similar concepts can be used to gauge the importance of sentences.\n",
    "\n",
    "### Step 1: Import Necessary Libraries\n",
    "\n",
    "Before we begin, let's import all the necessary libraries. If you haven't installed these libraries, you can do so using `pip`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805d62ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run this cell to install the necessary libraries\n",
    "#!pip install pdfminer.six numpy scipy networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ed5f9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\wasim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\wasim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import pdfminer\n",
    "from pdfminer.high_level import extract_text\n",
    "import numpy as np\n",
    "from scipy.sparse.csgraph import connected_components\n",
    "from scipy.sparse import csr_matrix\n",
    "import networkx as nx\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "nltk.download('punkt')  # Download the tokenizer model if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define function to extract text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    return extract_text(pdf_path)\n",
    "\n",
    "# Define function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    sentences = sent_tokenize(text)\n",
    "    preprocessed_sentences = []\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        filtered_words = [word for word in words if word.lower() not in stop_words and word.isalnum()]\n",
    "        preprocessed_sentences.append(' '.join(filtered_words))\n",
    "    return preprocessed_sentences\n",
    "\n",
    "# Define function to calculate sentence similarity\n",
    "def sentence_similarity(sent1, sent2):\n",
    "    vectorizer = TfidfVectorizer(min_df=1, stop_words='english')\n",
    "    \n",
    "    # Try-except block to handle the ValueError\n",
    "    try:\n",
    "        tfidf = vectorizer.fit_transform([sent1, sent2])\n",
    "        return ((tfidf * tfidf.T).A)[0, 1]\n",
    "    except ValueError:\n",
    "        # In case of an empty vocabulary, return 0 similarity\n",
    "        return 0.0\n",
    "\n",
    "# Define function to build the similarity graph\n",
    "def build_similarity_graph(sentences):\n",
    "    # Create an empty similarity matrix\n",
    "    S = np.zeros((len(sentences), len(sentences)))\n",
    "\n",
    "    # Populate the similarity matrix\n",
    "    for idx1, sentence1 in enumerate(sentences):\n",
    "        for idx2, sentence2 in enumerate(sentences):\n",
    "            if idx1 == idx2:\n",
    "                continue\n",
    "            S[idx1][idx2] = sentence_similarity(sentence1, sentence2)\n",
    "\n",
    "    # Convert the similarity matrix to a graph\n",
    "    graph = nx.from_numpy_array(S)\n",
    "    return graph\n",
    "\n",
    "# Define function to rank sentences using LexRank\n",
    "def lexrank_summarization(text, num_sentences=5):\n",
    "    # Preprocess the text\n",
    "    sentences = preprocess_text(text)\n",
    "\n",
    "    # Build the graph\n",
    "    graph = build_similarity_graph(sentences)\n",
    "\n",
    "    # Use the pagerank algorithm to rank sentences\n",
    "    scores = nx.pagerank(graph, max_iter=1000, tol=1e-06)\n",
    "    ranked_sentences = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)\n",
    "\n",
    "    # Extract top N sentences as the summary\n",
    "    summary = \" \".join([s[1] for s in ranked_sentences[:num_sentences]])\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0360e7ad",
   "metadata": {},
   "source": [
    "Now we will choose a PDF file and apply the summarization algorithm to extract the key points from the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "380750a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      " 1993 Max Weber conceptualization charismatic authority inﬂuence organizational research Leadership Quarterly Vol Findings Weber writings charismatic authority continue instrumental shaping modern leadership theory charismatic form authority may particularly applicable effective today chaotic rapidly changing environments empowered organizational forms century may represent merely different incarnation Weber iron cage authority short propose Weber writings charismatic authority continue instrumental shaping modern leadership theory charismatic form authority may particularly applicable effective today chaotic rapidly changing environments empowered organizational forms century may simply represent different embodiment Weber iron cage authority Weber forwarded three basic types authority traditional charismatic Weber 1968 Indeed concepts along aspects Weber theory charismatic authority recently prompted lively debate among leadership scholars within pages Leadership Quarterly Bass 1999 Downloaded Queen Mary University London 02 January 2019 PT JMH 452 Beyer 1999 House 1999 Shamir 1999\n"
     ]
    }
   ],
   "source": [
    "# Prompt the user to select a PDF file\n",
    "pdf_path = choose_file()\n",
    "\n",
    "# Extract text from the PDF\n",
    "text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# Generate summary\n",
    "summary = lexrank_summarization(text)\n",
    "\n",
    "# Print the summary\n",
    "print(\"Summary:\\n\", summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbdf959",
   "metadata": {},
   "source": [
    "## PDF Summarization Using Latent Semantic Analysis (LSA)\n",
    "\n",
    "Next we will demostrate how to summarise a PDF document using the Latent Semantic Analysis (LSA) approach. LSA is based on the singular value decomposition (SVD) of a term-sentence matrix to reduce its dimensions and thus identify patterns that represent the underlying \"latent\" structure of the semantic relationships within the text. By mapping the high-dimensional space of terms into a lower-dimensional space, LSA can infer the importance of sentences based on the concepts they contain, even if they do not share specific keywords.\n",
    "\n",
    "The summarization effect of LSA is to identify and extract sentences that carry the essence of the topics within the text. These sentences may not necessarily be the most frequently occurring or the most interconnected but rather those that best capture the main themes and variations in topic within the document.\n",
    "\n",
    "LSA is effective for complex texts with sophisticated structures, such as academic literature, research papers, and technical documents, where simple word frequency is insufficient to understand the importance of sentences. It is particularly useful for summarizing texts that require the identification of thematic importance and where synonymy and polysemy (words that have multiple meanings) are prevalent.\n",
    "\n",
    "### Step 1: Import Necessary Libraries\n",
    "\n",
    "Let's begin by importing all necessary libraries. Install them using pip if they are not already installed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8b36f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pdfminer.six numpy scipy scikit-learn nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02767713",
   "metadata": {},
   "source": [
    "### Step 2: Extract and Preprocess Text\n",
    "\n",
    "We need to preprocess the text by tokenizing sentences, removing stop words, and filtering non-alphanumeric characters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36f51253",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\wasim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\wasim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import pdfminer\n",
    "from pdfminer.high_level import extract_text\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt') # Download the tokenizer model if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define function to extract text from PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    return extract_text(pdf_path)\n",
    "\n",
    "# Define function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    sentences = sent_tokenize(text)\n",
    "    preprocessed_sentences = []\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        filtered_words = [word for word in words if word.lower() not in stop_words and word.isalnum()]\n",
    "        preprocessed_sentences.append(' '.join(filtered_words))\n",
    "    return preprocessed_sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac90d14",
   "metadata": {},
   "source": [
    "### Step 3: Implement LSA for Summarization\n",
    "\n",
    "LSA is used for summarization by applying dimensionality reduction to the term-sentence matrix and then extracting the sentences that contribute most to the resulting components.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2427b254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to perform LSA summarization\n",
    "def lsa_summarization(sentences, num_topics=1, num_sentences=5):\n",
    "    # Create a TfidfVectorizer for vectorization of the sentences\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "    # Perform SVD to reduce dimensionality\n",
    "    svd_model = TruncatedSVD(n_components=num_topics)\n",
    "    lsa = make_pipeline(svd_model, Normalizer(copy=False))\n",
    "    X_lsa = lsa.fit_transform(X)\n",
    "\n",
    "    # Rank sentences based on the weight in the first topic\n",
    "    ranked_sentences = [(sentence, X_lsa[index]) for index, sentence in enumerate(sentences)]\n",
    "    ranked_sentences = sorted(ranked_sentences, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Extract top N sentences as the summary\n",
    "    summary = \" \".join([sentence for sentence, weight in ranked_sentences[:num_sentences]])\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b3e16d",
   "metadata": {},
   "source": [
    "### Step 4: Display the Summary\n",
    "\n",
    "Finally, we display the summarized text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78304ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSA Summary:\n",
      " current issue full text archive journal available Max Weber notion authority still hold century Jeffery Houghton College Business Economics West Virginia University Morgantown West Virginia USA Max Weber notion authority 449 Abstract Purpose purpose brief commentary provide brief overview Max Weber life work contributions management thought addressing question whether notion authority still holds century commentary begins brief biographical sketch followed examination Weber conceptualization authority inﬂuence ﬁeld management relevancy century Findings Weber writings charismatic authority continue instrumental shaping modern leadership theory charismatic form authority may particularly applicable effective today chaotic rapidly changing environments empowered organizational forms century may represent merely different incarnation Weber iron cage authority commentary makes important contribution management history literature examining important aspect Weber inﬂuence management thought theory practice\n"
     ]
    }
   ],
   "source": [
    "# Prompt the user to select a PDF file\n",
    "pdf_path = choose_file()\n",
    "\n",
    "text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# Preprocess the text to be summarized\n",
    "preprocessed_text = preprocess_text(text)\n",
    "\n",
    "# Generate summary\n",
    "summary = lsa_summarization(preprocessed_text)\n",
    "\n",
    "# Generate summary\n",
    "print(\"LSA Summary:\\n\", summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8f668b",
   "metadata": {},
   "source": [
    "## PDF Summarization Using the Luhn Algorithm\n",
    "Luhn Algorithm algorithm is based on the frequency of words within the text; it assumes that words occurring more frequently are more significant. Luhn's insight was that there is a middle ground of word frequency that captures keywords: very common words are uninformative, and very rare words may be irrelevant. Furthermore, the Luhn algorithm pays attention to the proximity of these significant words to each other within a sentence, proposing that clusters of significant words are likely to convey more information.\n",
    "\n",
    "The summarization effect is that sentences that contain a higher density of these mid-frequency significant words, especially where they occur in close proximity, are selected for the summary. The resulting summary is therefore a collection of sentences that are rich in content-bearing words, which Luhn suggested represent the main points of the text.\n",
    "\n",
    " The Luhn algorithm is most effective for texts where the significant content can be identified through keyword frequency and distribution, such as news reports and business and technical papers. It works particularly well with texts that have a good signal-to-noise ratio in terms of keyword frequencies: significant words should stand out from the less important text while still being frequently enough used to indicate central themes.\n",
    "\n",
    "### Step 1: Import Necessary Libraries\n",
    "\n",
    "Before we begin, let's import all the necessary libraries. If you haven't installed these libraries, you can do so using pip.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0a59c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pdfminer.six nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2038a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\wasim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\wasim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pdfminer\n",
    "from pdfminer.high_level import extract_text\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f9a775",
   "metadata": {},
   "source": [
    "### Step 2: Extract Text from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41fdd1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    return extract_text(pdf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087801d2",
   "metadata": {},
   "source": [
    "### Step 3: Preprocess the Text\n",
    "We define a function to preprocess text by tokenizing sentences, removing stop words, and filtering non-alphanumeric characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ab556fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words and word.isalnum()]\n",
    "    \n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5e7606",
   "metadata": {},
   "source": [
    "### Step 4: Implement the Luhn Algorithm for Summarization\n",
    "The Luhn Algorithm scores sentences based on the frequency of significant words. Sentences with the highest scores are included in the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "23723dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def luhn_summarization(text, num_sentences=5):\n",
    "    sentences = sent_tokenize(text)\n",
    "    preprocessed_text = preprocess_text(text)\n",
    "    word_frequencies = FreqDist(word_tokenize(preprocessed_text))\n",
    "    \n",
    "    # Compute the higher frequency threshold using average frequency\n",
    "    avg_frequency = sum(word_frequencies.values()) / len(word_frequencies)\n",
    "    significant_words = {word for word in word_frequencies if word_frequencies[word] > avg_frequency}\n",
    "    \n",
    "    ranked_sentences = {}\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        words = word_tokenize(sentence.lower())\n",
    "        word_count = len(words)\n",
    "        score = sum([word_frequencies[word] for word in words if word in significant_words]) / word_count\n",
    "        ranked_sentences[i] = score\n",
    "    \n",
    "    selected_sentences = sorted(ranked_sentences, key=ranked_sentences.get, reverse=True)[:num_sentences]\n",
    "    summary = ' '.join([sentences[i] for i in selected_sentences])\n",
    "    \n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b364687",
   "metadata": {},
   "source": [
    "### Step 5: Display the Summary\n",
    "Finally, we display the summarized text after prompting the user to select a PDF file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "14ca8086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Luhn Summary:\n",
      " The ﬁrst is traditional authority. KEYWORDS authority,  bureaucratic  authority,  charisma,  dominance,  leadership,\n",
      "traditional authority, Weber\n",
      "\n",
      "Max Weber’s longstanding interest in the notion of authority is well documented. This authority is based upon strong traditional rules and has\n",
      "much  in  common  with  legal  authority. The charismatic leader should\n",
      "and often does have these traits. The charismatic leader is a d¨amonischer type who\n",
      "appears  only  in  chaotic  times.\n"
     ]
    }
   ],
   "source": [
    "# Prompt the user to select a PDF file\n",
    "pdf_path = choose_file()\n",
    "\n",
    "# Extract text from the PDF\n",
    "text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# Generate summary\n",
    "summary = luhn_summarization(text)\n",
    "\n",
    "# Print the summary\n",
    "print(\"Luhn Summary:\\n\", summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93df519a",
   "metadata": {},
   "source": [
    "## PDF Summarization Using KL-Sum\n",
    "KL-Sum algorithm is based on minimizing the Kullback-Leibler (KL) divergence, which is a way of measuring the difference between two probability distributions. For text summarization, KL-Sum aims to select sentences that, when taken together, provide a probability distribution of words as close as possible to the distribution in the original document. It iteratively adds sentences to the summary that most decrease the KL divergence.\n",
    "\n",
    "The effect of the KL-Sum algorithm is to create a summary that maintains the original distribution of words, which presumably represents the topics and the information content of the entire document. This tends to produce summaries that are representative of the original text's thematic structure.\n",
    "\n",
    "KL-Sum is effective for texts with distinct keyword distributions that are indicative of the text's content, such as scientific articles and technical documents. It can be especially useful in domains where the goal is to capture the key information without imposing much interpretation or paraphrasing, thus maintaining the document's original terminology and meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61cedcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy scipy nltk pymupdf nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4d3498",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c5c7ee3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\wasim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from scipy.special import kl_div\n",
    "from scipy.stats import entropy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from collections import Counter\n",
    "import fitz  \n",
    "\n",
    "# Ensure that you have the necessary NLTK models downloaded\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to extract text from a PDF file\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    # Open the PDF file\n",
    "    with fitz.open(pdf_path) as pdf:\n",
    "        text = \"\"\n",
    "        # Iterate over each page in the PDF\n",
    "        for page in pdf:\n",
    "            # Extract text from the page\n",
    "            text += page.get_text()\n",
    "        return text\n",
    "    \n",
    "# Function to extract sentences from a string of text\n",
    "def get_sentences(text):\n",
    "    return sent_tokenize(text)\n",
    "\n",
    "# Function to create the word frequency distribution\n",
    "def word_freq_dist(text):\n",
    "    words = word_tokenize(text.lower())\n",
    "    return Counter(words)\n",
    "\n",
    "# Function to calculate the KL divergence\n",
    "# Function to calculate the KL divergence\n",
    "def kl_divergence(summary_freq_dist, doc_freq_dist, vocab_size):\n",
    "    \"\"\"\n",
    "    Calculate the KL divergence, adding a small value (1/vocab_size) to zero-counts to avoid infinity.\n",
    "    \"\"\"\n",
    "    P = np.array([summary_freq_dist.get(word, 0) + 1.0/vocab_size for word in doc_freq_dist])\n",
    "    Q = np.array([doc_freq_dist.get(word, 0) + 1.0/vocab_size for word in doc_freq_dist])\n",
    "    \n",
    "    return entropy(P, Q)\n",
    "\n",
    "# Function to perform KL-Sum summarization\n",
    "# Function to perform KL-Sum summarization\n",
    "def kl_sum(text, summary_size=5):\n",
    "    sentences = get_sentences(text)\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.fit(sentences)\n",
    "    vocab = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    doc_freq_dist = word_freq_dist(text)\n",
    "    \n",
    "    summary = []\n",
    "    summary_freq_dist = Counter()\n",
    "    remaining_sentences = sentences.copy()\n",
    "    \n",
    "    vocab_size = len(vocab)  # V is the size of the vocabulary\n",
    "    \n",
    "    while len(summary) < summary_size and remaining_sentences:\n",
    "        kl_scores = []\n",
    "        for sentence in remaining_sentences:\n",
    "            temp_summary = ' '.join(summary + [sentence])\n",
    "            temp_summary_freq_dist = word_freq_dist(temp_summary)\n",
    "            kl_score = kl_divergence(temp_summary_freq_dist, doc_freq_dist, vocab_size)\n",
    "            kl_scores.append((kl_score, sentence))\n",
    "            \n",
    "        # Select the sentence that minimizes the KL divergence\n",
    "        min_kl_sentence = min(kl_scores, key=lambda x: x[0])[1]\n",
    "        summary.append(min_kl_sentence)\n",
    "        summary_freq_dist.update(word_freq_dist(min_kl_sentence))\n",
    "        remaining_sentences.remove(min_kl_sentence)\n",
    "    \n",
    "    return ' '.join(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e46a6718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Luhn Summary:\n",
      " 408-37. (1993), “Max Weber’s conceptualization of charismatic authority: its inﬂuence\n",
      "on organizational research”, Leadership Quarterly, Vol. Findings – Weber’s writings on charismatic authority have been and continue to be instrumental\n",
      "in shaping modern leadership theory, that the charismatic form of authority may be particularly applicable\n",
      "and effective in today’s chaotic and rapidly changing environments, and that the empowered and\n",
      "self-managing organizational forms of the twenty-ﬁrst century may represent merely a different incarnation\n",
      "of Weber’s iron cage of legal/rational authority. Indeed, these concepts along with other\n",
      "aspects of Weber’s theory of charismatic authority recently prompted a lively debate\n",
      "among leadership scholars within the pages of Leadership Quarterly (Bass, 1999;\n",
      "Max Weber’s\n",
      "notion of\n",
      "authority\n",
      "451\n",
      "Downloaded by Queen Mary University of London At 07:14 02 January 2019 (PT)\n",
      "Beyer, 1999; House, 1999; Shamir, 1999). Jeffery D. Houghton\n",
      "College of Business and Economics, West Virginia University,\n",
      "Morgantown, West Virginia, USA\n",
      "Abstract\n",
      "Purpose – The purpose of this brief commentary is to provide a brief overview of Max Weber’s life,\n",
      "work, and contributions to management thought before addressing the question of whether his notion\n",
      "of authority still holds in the twenty-ﬁrst century.\n"
     ]
    }
   ],
   "source": [
    "# Prompt the user to select a PDF file\n",
    "pdf_path = choose_file()\n",
    "\n",
    "# Extract text from the PDF\n",
    "text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# Generate summary\n",
    "summary = kl_sum(text)\n",
    "\n",
    "# Print the summary\n",
    "print(\"Luhn Summary:\\n\", summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1c557e",
   "metadata": {},
   "source": [
    "## PDF Summarization Using K-Mean Clustering\n",
    "This method breaks the document into smaller chunks, such as paragraphs, and then generates vector embeddings for each chunk. These embeddings capture the semantic meaning of the text in a multi-dimensional vector space, enabling the algorithm to measure the \"distance\" between chunks in terms of content and meaning. Paragraphs that are semantically similar will cluster together in this space. A clustering algorithm like K-means is then used to identify these clusters. The algorithm determines the central points of these clusters, which are the chunks that best represent the \"average meaning\" of the topics within each cluster.\n",
    "\n",
    "The approach essentially distills the document down to its key thematic elements. It identifies the main topics discussed throughout and selects the most representative sections for each topic. By combining the central chunks, a summary is created that is not only concise but also rich in context, reflecting the various key topics of the document.\n",
    "\n",
    "This method is versatile and can be effective across a range of text types. It is particularly useful for long documents such as reports, research papers, and lengthy articles where there are distinct sections or chapters. Since it relies on semantic understanding, it can handle complex materials where themes and ideas are more important than just individual keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd30ca09",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install numpy scikit-learn gensim pdfplumber \"unstructured[pdf]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12046226",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pip install \"unstructured[all-docs]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31049317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = ''\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            # Extract text\n",
    "            current_page_text = page.extract_text()\n",
    "            if current_page_text:  # Check if the text was extracted\n",
    "                text += current_page_text + \"\\n\\n\"  # Adding a double newline as a paragraph separator\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62e3610",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Prompt the user to select a PDF file\n",
    "pdf_path = choose_file()\n",
    "\n",
    "# After extracting text from the PDF\n",
    "text = extract_text_from_pdf(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75193509",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169cb26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "# Function to extract text from a PDF\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PdfReader(file)\n",
    "        text = ''\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "        return text\n",
    "\n",
    "# Summarization function\n",
    "def KMean_cluster_summerisation(document):\n",
    "    # Check the length of paragraphs after the split\n",
    "    paragraphs = document.split('\\n\\n')  # Adjust this line as needed based on the actual paragraph breaks\n",
    "    print(\"Number of paragraphs:\", len(paragraphs))\n",
    "\n",
    "    if len(paragraphs) < 2:\n",
    "        return \"Document is too short to summarize using clustering.\"\n",
    "\n",
    "    tagged_data = [TaggedDocument(words=par.split(), tags=[str(i)]) for i, par in enumerate(paragraphs)]\n",
    "    model = Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
    "    model.build_vocab(tagged_data)\n",
    "    model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    embeddings = [model.infer_vector(par.split()) for par in paragraphs]\n",
    "    optimal_k = min(len(paragraphs), 10)\n",
    "    clustering_model = KMeans(n_clusters=optimal_k)\n",
    "    clustering_model.fit(embeddings)\n",
    "    cluster_assignment = clustering_model.labels_\n",
    "    centroids = clustering_model.cluster_centers_\n",
    "    representative_paragraphs = []\n",
    "\n",
    "    for i in range(optimal_k):\n",
    "        centroid = centroids[i]\n",
    "        distances = np.linalg.norm(embeddings - centroid, axis=1)\n",
    "        cluster_paragraphs = [par for par, cluster in zip(paragraphs, cluster_assignment) if cluster == i]\n",
    "        representative_paragraphs.append(cluster_paragraphs[np.argmin(distances)])\n",
    "    sorted_representative_paragraphs = sorted(representative_paragraphs, key=lambda par: paragraphs.index(par))\n",
    "    summary = '\\n\\n'.join(sorted_representative_paragraphs)\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fec21e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary\n",
    "summary = KMean_cluster_summerisation(text)\n",
    "\n",
    "# Print the summary\n",
    "print(\"K-Mean Clustering Summary:\\n\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f6a83a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
